{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d00aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import Conv2D, PReLU, BatchNormalization,Flatten\n",
    "from keras.layers import UpSampling2D, LeakyReLU, Dense, Input, add\n",
    "from tqdm import tqdm #progess bar\n",
    "from keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5172f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_dir = \"H:/python3/pythonProject/venv/AIP_Project/Gan_SuperRes/Gan_SuperRes/train_dir\"\n",
    "\n",
    "# for img in os.listdir(train_dir +\"/original_images/mirflickr25k/mirflickr\"):\n",
    "#     img_array = cv.imread(train_dir + \"/original_images/mirflickr25k/mirflickr/\" + img)\n",
    "\n",
    "#     img_array = cv.resize(img_array, (256,256))\n",
    "#     lr_img_array = cv.GaussianBlur(img_array,(5,5),0)\n",
    "#     lr_img_array = cv.resize(img_array,(64,64))\n",
    "#     cv.imwrite(train_dir+\"/hr_images/\"+img,img_array)\n",
    "#     cv.imwrite(train_dir+\"/lr_images/\"+img,lr_img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37161fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator\n",
    "\n",
    "def res_block(ip):\n",
    "    res_model = Conv2D(64,(5,5), padding=\"same\")(ip)\n",
    "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
    "    res_model = Conv2D(64, (3,3), padding = \"same\")(res_model)\n",
    "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
    "    res_model = PReLU(shared_axes = [1,2])(res_model)\n",
    "\n",
    "    res_model = Conv2D(64, (3,3), padding = \"same\")(res_model)\n",
    "    res_model = BatchNormalization(momentum = 0.5)(res_model)\n",
    "\n",
    "    return add([ip, res_model])\n",
    "\n",
    "def upscale_block(ip):\n",
    "\n",
    "    up_model = Conv2D(256, (3,3), padding=\"same\")(ip)\n",
    "    up_model = PReLU(shared_axes=[1,2])(up_model)\n",
    "    up_model = Conv2D(256, (3,3), padding=\"same\")(up_model)\n",
    "    up_model = BatchNormalization(momentum=0.5)(up_model)\n",
    "    up_model = UpSampling2D(size=2)(up_model)\n",
    "    up_model = PReLU(shared_axes=[1,2])(up_model)\n",
    "    return up_model\n",
    "\n",
    "def create_gen(gen_ip, num_res_block):\n",
    "    layers = Conv2D(64, (9,9),padding = \"same\")(gen_ip)\n",
    "    layers = PReLU(shared_axes=[1,2])(layers)\n",
    "    layers = Conv2D(64, (5,5),padding = \"same\")(layers)\n",
    "    layers = PReLU(shared_axes=[1,2])(layers)\n",
    "\n",
    "\n",
    "    temp = layers\n",
    "\n",
    "    for i in range(num_res_block):\n",
    "        layers = res_block(layers)\n",
    "\n",
    "    layers = Conv2D(64, (3,3), padding=\"same\")(layers)\n",
    "    layers = BatchNormalization(momentum=0.5)(layers)\n",
    "    layers = add([layers,temp])\n",
    "\n",
    "    layers = upscale_block(layers)\n",
    "    layers = upscale_block(layers)\n",
    "\n",
    "    op = Conv2D(3, (9,9), padding=\"same\")(layers)\n",
    "\n",
    "    return Model(inputs=gen_ip, outputs=op)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119bdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "def discriminator_block(ip, filters, strides=1, bn=True):\n",
    "    disc_model = Conv2D(filters, (3,3) ,strides = strides, padding=\"same\")(ip)\n",
    "\n",
    "    if bn:\n",
    "        disc_model = BatchNormalization( momentum=0.8)(disc_model)\n",
    "\n",
    "    disc_model = LeakyReLU( alpha=0.3)(disc_model)\n",
    "\n",
    "    return disc_model\n",
    "\n",
    "def create_disc(disc_ip):\n",
    "    df = 40\n",
    "\n",
    "    d1 = discriminator_block(disc_ip, df , bn = False)\n",
    "    d2 = discriminator_block(d1, df, strides=2)\n",
    "    d3 = discriminator_block(d2, df*2)\n",
    "    d4 = discriminator_block(d3, df*2, strides = 2)\n",
    "    d5 = discriminator_block(d4, df*4)\n",
    "    d6 = discriminator_block(d5, df*4, strides=2)\n",
    "    d7 = discriminator_block(d6, df*8)\n",
    "    d8 = discriminator_block(d7, df*8, strides=2)\n",
    "\n",
    "    d8_5 = Flatten()(d8)\n",
    "    d9 = Dense(df*16)(d8_5)\n",
    "    d10 = LeakyReLU(alpha=0.3)(d9)\n",
    "    validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "    return Model(disc_ip, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination\n",
    "\n",
    "def build_vgg(hr_shape):\n",
    "\n",
    "    vgg = VGG19(weights=\"imagenet\",include_top=False, input_shape=hr_shape)\n",
    "\n",
    "    return Model(inputs=vgg.inputs, outputs=vgg.layers[10].output)\n",
    "\n",
    "def create_comb(gen_model, disc_model, vgg, lr_ip, hr_ip):\n",
    "    gen_img = gen_model(lr_ip)\n",
    "    gen_features = vgg(gen_img)\n",
    "\n",
    "    disc_model.trainable = False\n",
    "    validity = disc_model(gen_img)\n",
    "\n",
    "    return Model(inputs=[lr_ip, hr_ip], outputs=[validity, gen_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24db25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "n=25000\n",
    "lr_list = os.listdir(train_dir + \"lr_images\")[:n]\n",
    "\n",
    "# lr_images = []\n",
    "# for img in lr_list:\n",
    "#     img_lr = cv2.imread(\"lr_images/\"+img)\n",
    "#     img_lr = cv2.cvtColor(img_lr, cv2.COLOR_BGR2RGB)\n",
    "#     lr_images.append(img_lr)\n",
    "lr_image_urls = []\n",
    "for url in lr_list:\n",
    "    lr_image_urls.append(str(\"lr_images/\"+url))\n",
    "\n",
    "hr_list = os.listdir(train_dir + \"hr_images\")[:n]\n",
    "\n",
    "# hr_images = []\n",
    "# for img in hr_list:\n",
    "#     img_hr = cv2.imread(\"hr_images/\"+img)\n",
    "#     img_hr = cv2.cvtColor(img_hr, cv2.COLOR_BGR2RGB)\n",
    "#     hr_images.append(img_hr)\n",
    "hr_image_urls = []\n",
    "for url in lr_list:\n",
    "    hr_image_urls.append(str(\"hr_images/\"+url))\n",
    "\n",
    "# lr_images = np.array(lr_images)\n",
    "# hr_images = np.array(hr_images)\n",
    "\n",
    "#Plot random lr and hr image\n",
    "# import random\n",
    "# import numpy as np\n",
    "# image_number = random.randint(0,len(lr_images)-1)\n",
    "# plt.figure(figsize=(12,6))\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(np.reshape(lr_images[image_number],(32,32,3)))\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(np.reshape(hr_images[image_number],(128,128,3)))\n",
    "# plt.show()\n",
    "\n",
    "# lr_images = lr_images/255\n",
    "# hr_images = hr_images/255\n",
    "\n",
    "lr_train, lr_test, hr_train, hr_test = train_test_split(lr_image_urls, hr_image_urls,test_size=0.01,random_state=True)\n",
    "\n",
    "# hr_shape = (hr_train.shape[1], hr_train.shape[2], hr_train.shape[3])\n",
    "# lr_shape = (lr_train.shape[1], lr_train.shape[2], lr_train.shape[3])\n",
    "\n",
    "hr_shape = (256,256,3)\n",
    "lr_shape = (64,64,3)\n",
    "\n",
    "lr_ip = Input(shape=lr_shape)\n",
    "hr_ip = Input(shape=hr_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "generator = create_gen(lr_ip, num_res_block=20)\n",
    "generator.summary()\n",
    "\n",
    "discriminator = create_disc(hr_ip)\n",
    "discriminator.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "discriminator.summary()\n",
    "\n",
    "vgg = build_vgg((256,256,3))\n",
    "print(vgg.summary())\n",
    "vgg.trainable = False\n",
    "\n",
    "gan_model = create_comb(generator,discriminator,vgg,lr_ip,hr_ip)\n",
    "\n",
    "gan_model.compile(loss=[\"binary_crossentropy\",\"mse\"], loss_weights=[1e-3,1],optimizer=\"adam\")\n",
    "gan_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb3e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "# Custom batch of data\n",
    "batch_size = 1\n",
    "train_lr_batches = []\n",
    "train_hr_batches = []\n",
    "\n",
    "for it in range(int(len(hr_train)/batch_size)):\n",
    "    start_idx = it * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "    train_hr_batches.append(hr_train[start_idx:end_idx])\n",
    "    train_lr_batches.append(lr_train[start_idx:end_idx])\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    fake_label = np.zeros((batch_size,1))\n",
    "    real_label = np.ones((batch_size,1))\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "    for b in tqdm(range(len(train_hr_batches))):\n",
    "      #......\n",
    "        lr_image_urls = train_lr_batches[b]\n",
    "        lr_images = cv2.imread(\"\".join(lr_image_urls))\n",
    "        lr_images = np.array(lr_images)\n",
    "        lr_images = cv2.cvtColor(lr_images, cv2.COLOR_BGR2RGB)\n",
    "        lr_images = lr_images/255\n",
    "\n",
    "        hr_image_urls = train_hr_batches[b]\n",
    "        hr_images = cv2.imread(\"\".join(hr_image_urls))\n",
    "        hr_images = np.array(hr_images)\n",
    "        hr_images = cv2.cvtColor(hr_images, cv2.COLOR_BGR2RGB)\n",
    "        hr_images = hr_images/255\n",
    "\n",
    "        lr_images = np.expand_dims(lr_images, axis=0)\n",
    "        hr_images = np.expand_dims(hr_images,axis=0)\n",
    "#...\n",
    "        fake_imgs = generator.predict_on_batch(lr_images)\n",
    "\n",
    "        discriminator.trainable = True\n",
    "        d_losses_gen = discriminator.train_on_batch(fake_imgs,fake_label)\n",
    "        d_losses_real = discriminator.train_on_batch(hr_images,real_label)\n",
    "\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        d_loss = 0.5 + np.add(d_losses_gen, d_losses_real)\n",
    "        image_features = vgg.predict(hr_images)\n",
    "\n",
    "        g_loss, _, _ = gan_model.train_on_batch([lr_images, hr_images],[real_label, image_features])\n",
    "\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "    g_losses = np.array(g_losses)\n",
    "    d_losses = np.array(d_losses)\n",
    "\n",
    "    g_loss = np.sum(g_losses, axis=0)/ len(g_losses)\n",
    "    d_loss = np.sum(d_losses,axis=0) / len(d_losses)\n",
    "\n",
    "    print(\"epoch:\", e+1, \"g_loss:\", g_loss, \"d_loss:\", d_loss)\n",
    "\n",
    "    generator.save(\"gen_e_\"+str(e+1)+\" \"+str(g_loss)+\" \"+str(n)+\" \"+str(d_loss)+\".h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Imageflow\n",
    "# filepath = \"Treeweights-{epoch:82d}-{val_accuracy:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath,monitor='val_accuracy',verbose=1,save_best_only=True,mode='max')\n",
    "# callbacks_list = [checkpoint]\n",
    "#\n",
    "# aug = ImageDataGenerator(rotation_range=20, zoom_range=0.2,\n",
    "#      rescale=1./255,\n",
    "#     width_shift_range=0.1,\n",
    "#       height_shift_range=0.1,\n",
    "#   \thorizontal_flip=True,\n",
    "#      brightness_range=[0.5,2.5], fill_mode=\"nearest\")\n",
    "#\n",
    "# aug_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#Fit_model\n",
    "# vgghist=vggmodel.fit_generator(aug.flow(x_train, y_train, batch_size=64),\n",
    "#                                  epochs=50,\n",
    "#                                  validation_data=aug.flow(x_test,y_test,\n",
    "#                                  batch_size=64),\n",
    "#                                  callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "#Load weights\n",
    "# generator = create_gen(lr_ip, num_res_block=16)\n",
    "# generator.summary()\n",
    "#\n",
    "# discriminator = create_disc(hr_ip)\n",
    "# discriminator.compile(loss=\"binary_crossentopy\",optimizer=\"adam\",metrics=['accuracy'])\n",
    "# discriminator.summary()\n",
    "#\n",
    "# vgg = build_vgg((128,128,3))\n",
    "# print(vgg.summary())\n",
    "# vgg.trainable = False\n",
    "#\n",
    "# gan_model = create_comb(generator,discriminator,vgg,lr_ip,hr_ip)\n",
    "# gan_model.load_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba0beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)dasd",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
